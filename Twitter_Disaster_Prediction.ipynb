{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a233cb17",
   "metadata": {},
   "source": [
    "### This Jupyter Notebook is my submission to the Kaggle Twitter disaster prediction challenge where we were given the tweets and locatio of the tweets and we had to perform NLP on the tweets to check if the disaster is real or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec656c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import torch # Used for deep learning\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "# DistilBertTokenizer performs tokenization which breaks down input text into smaller units, such as words or subwords, and converts them into numerical representations\n",
    "# DistilBertForSequenceClassification is designed to handle tasks where the model needs to classify the input text into different categories or classes\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "# DataLoader can define how the dataset should be loaded, specify the batch size, enable shuffling of the data for randomness, utilize multi-process data loading for faster training\n",
    "# Dataset acts like a blueprint for creating custom datasets in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7010cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d51fe188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining custom dataset class\n",
    "class DisasterTweetsDataset(Dataset): # Creating a new class which is subset of Dataset class from pytorch\n",
    "    def __init__(self, data, tokenizer, max_length, is_test = False): # Constructor of the class \n",
    "        self.data = data # The input dataset\n",
    "        self.tokenizer = tokenizer # An instance of tokenizer, like DistilBertTokenizer, used to tokenize the tweet text\n",
    "        self.max_length = max_length # The max length of the tokenized input, tweets longer than this length will be truncated\n",
    "        self.is_test = is_test # Indicating whether the dataset is for testing or not\n",
    "        \n",
    "    def __len__(self): # Returns the length of the dataset\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index): # Tokenizes the tweet text using the tokenizer and retrieves the input tensors \n",
    "        tweet = self.data['text'][index] # Get the tweet text from the data at the specified index\n",
    "        encoding = self.tokenizer.encode_plus( # Using the tokenizer's encode_plus method to tokenize the tweet text\n",
    "            tweet,\n",
    "            add_special_tokens = True, # Adds special tokens like [CLS] (classification) and [SEP] (separator) tokens\n",
    "            max_length = self.max_length, # Truncates or pads the tokenized input to the specified max_length\n",
    "            padding = 'max_length', # Pads the sequences to have the same length as max_length\n",
    "            truncation = True, # Truncates the sequences if they exceed max_length\n",
    "            return_tensors = 'pt' # Returns the tokenized inputs as PyTorch tensors\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze() # Represent the tokenized input sequence, where each token is mapped to its corresponding ID\n",
    "        attention_mask = encoding['attention_mask'].squeeze() # A binary mask indicating which tokens should be attended to (1) and which should be ignored (0) during processing\n",
    "        \n",
    "        if self.is_test: # Depending on the is_test flag, returns the appropriate dictionary\n",
    "            return { # If is_test is True, only the tokenized inputs are returned in a dictionary\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask\n",
    "            }\n",
    "        else: # If is_test is False, the label corresponding to the tweet is retrieved, converted to a PyTorch tensor and returned in addition to the tokenized inputs in a dictionary\n",
    "            label = torch.tensor(self.data['target'][index])\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'label': label\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82dd3d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#  Setting up the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# DistilBertTokenizer class is responsible for tokenizing the input text into tokens that can be understood by the DistilBERT model\n",
    "# The from_pretrained method loads the tokenizer with the pre-trained weights from the 'distilbert-base-uncased' model, which is a version of DistilBERT trained on uncased English text\n",
    "# Using this tokenizer, we can convert text inputs into tokenized representations suitable for input to the DistilBERT model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 2)\n",
    "# This line creates an instance of the DistilBertForSequenceClassification class, which is a pre-trained DistilBERT model fine-tuned for sequence classification tasks\n",
    "# The num_labels parameter is set to 2, indicating that the model is configured for a binary classification task (two labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f14616ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting device and defining training parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # This line assigns the device on which the model will be trained\n",
    "# Assigning the device allows the model to utilize the available hardware for faster training and inference\n",
    "\n",
    "batch_size = 16 # Determines the number of samples processed simultaneously during training or inference\n",
    "max_length = 128 # Sets the maximum length of the input sequences\n",
    "num_epochs = 50 # Determines how many times the entire dataset will be passed through the model during training\n",
    "learning_rate = 2e-5 # Controls the step size or the rate at which the model's weights are updated during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23e93085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train dataset and loader\n",
    "train_dataset = DisasterTweetsDataset(train_data, tokenizer, max_length) \n",
    "# This line creates an instance of the DisasterTweetsDataset class, which is a custom dataset specifically designed for handling disaster tweets data\n",
    "# This dataset will be used to feed the training samples to the model during the training process\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "# The DataLoader class is provided by PyTorch and is responsible for loading the data in batches during training\n",
    "# The shuffle parameter indicates that the training samples will be shuffled randomly before being divided into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b1b3f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  9152, 23033,  ...,     0,     0,     0],\n",
      "        [  101,  1030,  2534,  ...,     0,     0,     0],\n",
      "        [  101,  3796,  1996,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 16129,  2629,  ...,     0,     0,     0],\n",
      "        [  101,  2023,  8505,  ...,     0,     0,     0],\n",
      "        [  101,  1030,  5553,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'label': tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "# Sanity check of train loader\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break\n",
    "# Shows how to use the train_loader data loader to iterate over the batches of training data\n",
    "# It prints the contents of the first batch to give you an idea of the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7068e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushing model to device along with defining optimizer and loss function\n",
    "model.to(device) # Moves the model to the specified device, ensures the model and its operations will be performed on the selected device during training\n",
    "model.train() #  Activating the training-specific behavior of the model, enabling features like dropout and batch normalization\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate) # Initializes the optimizer, which is responsible for updating the model's parameters during training to minimize the loss\n",
    "# The AdamW optimizer is used, which incorporates weight decay (L2 regularization) to help prevent overfitting\n",
    "# model.parameters() method returns an iterable of the model's learnable parameters that need to be optimized\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # Initializes the loss function that will be used to compute the model's training loss\n",
    "# CrossEntropyLoss is commonly used for multi-class classification problems, where the input consists of class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c652be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 206.2861\n",
      "Epoch 2/50, Loss: 151.8810\n"
     ]
    }
   ],
   "source": [
    "# Fine tuning the model for 5 epochs\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device) # Extracting the input data (input_ids and attention_mask) and the corresponding labels from the current batch\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad() # Clearing the gradients of all optimized parameters. It is necessary to reset the gradients before computing the gradients for the current batch to avoid accumulating gradients from previous batches\n",
    "\n",
    "        outputs = model(input_ids, attention_mask = attention_mask, labels=labels) # Passeing the input data and labels to the model\n",
    "        loss = outputs.loss # Extracting the loss value and predicted logits from the outputs object for the current batch\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_loss += loss.item() # Adding the current batch's loss value to the total_loss variable, accumulating the loss for the current epoch\n",
    "        # .item() method is used to obtain the loss value as a scalar instead of a tensor\n",
    "        loss.backward() # Performing backpropagation to calculate how the loss value changes as the model's parameters change\n",
    "        optimizer.step() # Performing a parameter update step to minimize the loss\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14358a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating test dataset\n",
    "test_dataset = DisasterTweetsDataset(test_data, tokenizer, max_length, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushing model to eval mode\n",
    "model.eval() # Setting the model to evaluation mode. It's important to switch to evaluation mode before making predictions or evaluating the model's performance\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions\n",
    "from tqdm import tqdm # Providing a progress bar for loops allowing us to track the progress of the loop and provides an estimate of the remaining time\n",
    "with torch.no_grad(): # It's used during evaluation to disable gradient computation which reduces memory usage and speeds up the process\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask) # Passing the input data and attention mask to the model to obtain the model's outputs\n",
    "        logits = outputs.logits # Extracting the predicted logits from the outputs object.\n",
    "        predicted_labels = torch.argmax(logits, dim=1).cpu().numpy() # Calculating the predicted labels by taking the index of the maximum value along the second dimension of the logits tensor\n",
    "        # argmax() function returns the index with the highest value for each sample in the batch and .cpu().numpy() converts the tensor to a NumPy array on the CPU\n",
    "        predictions.extend(predicted_labels.tolist()) # Extending the predictions list with the predicted labels for the current batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80555a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating submission file\n",
    "submission_data = {'id': test_data['id'], 'target': predictions}\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf1303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
